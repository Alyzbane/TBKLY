E:\venv\ml\Lib\site-packages\transformers\models\vit\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  context_layer = torch.nn.functional.scaled_dot_product_attention(
***** train metrics *****
  epoch                    =        1.0
  total_flos               = 17827277GF
  train_loss               =     1.1963
  train_runtime            = 0:03:29.51
  train_samples_per_second =      1.179
  train_steps_per_second   =      0.038
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =     0.8065
  eval_loss               =     0.9894
  eval_runtime            = 0:00:16.27
  eval_samples_per_second =      1.905
  eval_steps_per_second   =      0.061
***** train metrics *****
  epoch                    =        1.0
  total_flos               = 17827277GF
  train_loss               =     0.9747
  train_runtime            = 0:03:27.52
  train_samples_per_second =       1.19
  train_steps_per_second   =      0.039
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
***** train metrics *****
  epoch                    =        1.0
  total_flos               = 17827277GF
  train_loss               =     0.6372
  train_runtime            = 0:04:25.43
  train_samples_per_second =      0.931
  train_steps_per_second   =       0.03
***** train metrics *****
  epoch                    =        1.0
  total_flos               = 17827277GF
  train_loss               =     0.5676
  train_runtime            = 0:03:40.07
  train_samples_per_second =      1.122
  train_steps_per_second   =      0.036