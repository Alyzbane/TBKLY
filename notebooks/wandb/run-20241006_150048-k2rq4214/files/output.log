E:\venv\ml\Lib\site-packages\transformers\models\vit\modeling_vit.py:261: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  context_layer = torch.nn.functional.scaled_dot_product_attention(
KeyboardInterrupt
Cache path set on Y:/.cache/
Token is valid (permission: write).
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
Your token has been saved in your configured git credential helpers (manager,store).
Your token has been saved to Y:/.cache/token
Login successful
Path exist Y:\ML\datasets\barks\Barkley
Path exist Y:\ML\datasets\barks\Barkley
Class weights:  [0.99737705 0.85449438 0.98766234 1.10217391 1.10217391]
Class weights tensor:  tensor([0.9974, 0.8545, 0.9877, 1.1022, 1.1022], device='cuda:0')
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Path exist Y:\ML\datasets\barks\data
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
KeyboardInterrupt
Cache path set on Y:/.cache/
Token is valid (permission: write).
Your token has been saved in your configured git credential helpers (manager,store).
Your token has been saved to Y:/.cache/token
Login successful
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
Path exist Y:\ML\datasets\barks\data
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Exception ignored in: <function Dataset.__del__ at 0x000001E1456B94E0>
Traceback (most recent call last):
  File "E:\venv\ml\Lib\site-packages\datasets\arrow_dataset.py", line 1396, in __del__
    if hasattr(self, "_data"):
       ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
{0: 1.2875, 1: 1.03, 2: 0.965625, 3: 1.03, 4: 1.03, 5: 0.8828571428571429, 6: 0.9967741935483871, 7: 1.03, 8: 0.965625, 9: 0.8828571428571429}
{0: 1.2875, 1: 1.03, 2: 0.965625, 3: 1.03, 4: 1.03, 5: 0.8828571428571429, 6: 0.9967741935483871, 7: 1.03, 8: 0.965625, 9: 0.8828571428571429}
{0: 1.2875, 1: 1.03, 2: 0.965625, 3: 1.03, 4: 1.03, 5: 0.8828571428571429, 6: 0.9967741935483871, 7: 1.03, 8: 0.965625, 9: 0.8828571428571429}
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\MASTER\AppData\Local\Temp\ipykernel_43904\1423503731.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.class_weights = torch.tensor(class_weights, dtype=torch.float)
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Path exist Y:\ML\datasets\barks\data
{0: 1.2875, 1: 1.03, 2: 0.965625, 3: 1.03, 4: 1.03, 5: 0.8828571428571429, 6: 0.9967741935483871, 7: 1.03, 8: 0.965625, 9: 0.8828571428571429}
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
