{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d68e733",
   "metadata": {},
   "source": [
    "## Setting Up Your Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2ee377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Install PyTorch with CUDA\n",
    "# !pip install -q torch torchvision torchaudio\n",
    "\n",
    "# # Install additional dependencies\n",
    "# !pip install -q datasets matplotlib pandas pillow timm torcheval torchtnt==0.2.0 tqdm\n",
    "\n",
    "# # Install utility packages\n",
    "# !pip install -q cjm_pandas_utils cjm_pil_utils cjm_pytorch_utils cjm_torchvision_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494ad6c-0db9-4d9f-a1a2-89fb544e8053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d9dea7",
   "metadata": {},
   "source": [
    "## Importing the Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "afbf483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "from copy import copy\n",
    "import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_pandas_utils.core import markdown_to_pandas\n",
    "from cjm_pil_utils.core import resize_img, get_img_files\n",
    "from cjm_psl_utils.core import download_file, file_extract\n",
    "from cjm_pytorch_utils.core import set_seed, pil_to_tensor, tensor_to_pil, get_torch_device, denorm_img_tensor\n",
    "from cjm_torchvision_tfms.core import ResizeMax, PadSquare\n",
    "\n",
    "# Import matplotlib for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Import numpy \n",
    "import numpy as np\n",
    "\n",
    "# Import pandas module for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Do not truncate the contents of cells and display all rows and columns\n",
    "pd.set_option('max_colwidth', None, 'display.max_rows', None, 'display.max_columns', None)\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Import timm library\n",
    "import timm\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2  as transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "from torchtnt.utils import get_module_summary\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55d2cf",
   "metadata": {},
   "source": [
    "## Setting Up the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db2abe",
   "metadata": {},
   "source": [
    "### Setting a Random Number Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f944d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.\n",
    "seed = 1234\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bffa28",
   "metadata": {},
   "source": [
    "### Setting the Device and Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10990720",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_torch_device()\n",
    "dtype = torch.float32\n",
    "device, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d352f",
   "metadata": {},
   "source": [
    "### Setting the Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ede663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name for the project\n",
    "project_name = f\"pytorch-timm-image-classifier\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store datasets\n",
    "dataset_dir = Path(\"Datasets/\")\n",
    "# Create the dataset directory if it does not exist\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store archive files\n",
    "archive_dir = dataset_dir/'../Archive'\n",
    "# Create the archive directory if it does not exist\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Creating a Series with the paths and converting it to a DataFrame for display\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir,\n",
    "    \"Dataset Directory:\": dataset_dir, \n",
    "    \"Archive Directory:\": archive_dir\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9313246",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e95d3",
   "metadata": {},
   "source": [
    "### Setting the Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfacdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the dataset\n",
    "dataset_name = 'barks'\n",
    "\n",
    "# Construct the HuggingFace Hub dataset name by combining the username and dataset name\n",
    "hf_dataset = f'cj-mills/{dataset_name}'\n",
    "\n",
    "# Create the path to the zip file that contains the dataset\n",
    "archive_path = Path(f'{archive_dir}/{dataset_name.removesuffix(\"-zip\")}.zip')\n",
    "\n",
    "# Create the path to the directory where the dataset will be extracted\n",
    "dataset_path = Path(f'{dataset_dir}/{dataset_name.removesuffix(\"-zip\")}')\n",
    "\n",
    "# Creating a Series with the dataset name and paths and converting it to a DataFrame for display\n",
    "pd.Series({\n",
    "    \"HuggingFace Dataset:\": hf_dataset, \n",
    "    \"Archive Path:\": archive_path, \n",
    "    \"Dataset Path:\": dataset_path\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b730f8",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "# Function to extract .zip and .tar.gz files\n",
    "def file_extract(fname, dest):\n",
    "    if fname.suffix == \".zip\":\n",
    "        with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest)\n",
    "    elif fname.suffixes == ['.tar', '.gz'] or fname.suffix == '.tar':\n",
    "        with tarfile.open(fname, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(dest)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {fname.suffixes}\")\n",
    "\n",
    "# Construct the HuggingFace Hub dataset URL\n",
    "dataset_url = \"https://huggingface.co/datasets/alyzbane/barks/resolve/main/barks.tar.gz\"\n",
    "print(f\"HuggingFace Dataset URL: {dataset_url}\")\n",
    "\n",
    "# Set whether to delete the archive file after extracting the dataset\n",
    "delete_archive = True\n",
    "\n",
    "# Download the dataset if not present\n",
    "if dataset_path.is_dir():\n",
    "    print(\"Dataset folder already exists\")\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_file(dataset_url, archive_path)  # Assuming download_file is defined\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    file_extract(fname=archive_path, dest=dataset_dir)  # Extract the .tar.gz file\n",
    "    \n",
    "    # Delete the archive if specified\n",
    "    if delete_archive: archive_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724268ed-89d6-4c46-8af5-d5d25a7ce10f",
   "metadata": {},
   "source": [
    "### Get Image Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cd435-78a0-4c41-8c4c-fdf8bd5bee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(r\"Y:\\ML\\datasets\\barks\\Barkley\")\n",
    "img_folder_paths = [folder for folder in dataset_path.iterdir() if folder.is_dir()]\n",
    "\n",
    "# Display the names of the folders using a Pandas DataFrame\n",
    "pd.DataFrame({\"Image Folder\": [folder.name for folder in img_folder_paths]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f7ccb-98e4-4935-b380-e68a38d79749",
   "metadata": {},
   "source": [
    "### Get Image File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82f5c1-e913-43bb-8791-267855ae2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all image file paths from the image folders\n",
    "class_file_paths = [get_img_files(folder) for folder in img_folder_paths]\n",
    "\n",
    "# Get all image files in the 'img_dir' directory\n",
    "img_paths = [\n",
    "    file\n",
    "    for folder in class_file_paths # Iterate through each image folder\n",
    "    for file in folder # Get a list of image files in each image folder\n",
    "]\n",
    "\n",
    "# Print the number of image files\n",
    "print(f\"Number of Images: {len(img_paths)}\")\n",
    "\n",
    "# Display the first five entries using a Pandas DataFrame\n",
    "pd.DataFrame(img_paths).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80a4d6",
   "metadata": {},
   "source": [
    "### Inspecting the Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e96f",
   "metadata": {},
   "source": [
    "#### Get image classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df9c43-965f-4f8c-99d0-df96d0479043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples for each image class\n",
    "class_counts_dict = {folder[0].parent.name:len(folder) for folder in class_file_paths}\n",
    "\n",
    "# Get a list of unique labels\n",
    "class_names = list(class_counts_dict.keys())\n",
    "\n",
    "# Display the labels and the corresponding number of samples using a Pandas DataFrame\n",
    "class_counts = pd.DataFrame.from_dict({'Count':class_counts_dict})\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013adba",
   "metadata": {},
   "source": [
    "#### Visualize the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Classes')\n",
    "plt.xticks(range(len(class_counts.index)), class_names)  # Set the x-axis tick labels\n",
    "plt.xticks(rotation=75)  # Rotate x-axis labels\n",
    "plt.gca().legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7de4b2",
   "metadata": {},
   "source": [
    "### Visualizing Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the first image found for each class\n",
    "sample_image_paths = [folder[0] for folder in class_file_paths]\n",
    "sample_labels = [path.parent.stem for path in sample_image_paths]\n",
    "\n",
    "# Calculate the number of rows and columns\n",
    "grid_size = math.floor(math.sqrt(len(sample_image_paths)))\n",
    "n_rows = grid_size+(1 if grid_size**2 < len(sample_image_paths) else 0)\n",
    "n_cols = grid_size\n",
    "\n",
    "# Create a figure for the grid\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(12,12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # If we have an image for this subplot\n",
    "    if i < len(sample_image_paths) and sample_image_paths[i]:\n",
    "        # Add the image to the subplot\n",
    "        ax.imshow(np.array(Image.open(sample_image_paths[i])))\n",
    "        # Set the title to the corresponding class name\n",
    "        ax.set_title(sample_labels[i])\n",
    "        # Remove the axis\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        # If no image, hide the subplot\n",
    "        ax.axis('off')\n",
    "\n",
    "# Display the grid\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ff774-d4e5-4dcc-8bf9-86fed915c348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1035348f",
   "metadata": {},
   "source": [
    "## Selecting a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74614f12",
   "metadata": {},
   "source": [
    "### Exploring Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53993c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(timm.list_models('resnet18*', pretrained=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac47bf",
   "metadata": {},
   "source": [
    "### Inspecting the Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the resnet module\n",
    "from timm.models import resnet as model_family\n",
    "\n",
    "# Define the base model variant to use\n",
    "base_model = 'resnet18'\n",
    "version = \"a1_in1k\"\n",
    "\n",
    "# Get the default configuration of the chosen model\n",
    "model_cfg = model_family.default_cfgs[base_model].default.to_dict()\n",
    "\n",
    "# Show the default configuration values\n",
    "pd.DataFrame.from_dict(model_cfg, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8884c",
   "metadata": {},
   "source": [
    "### Retrieving Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve normalization statistics (mean and std) specific to the pretrained model\n",
    "mean, std = model_cfg['mean'], model_cfg['std']\n",
    "norm_stats = (mean, std)\n",
    "norm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff0ee8",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "85fc7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pretrained ResNet model with the number of output classes equal to the number of class names\n",
    "# 'timm.create_model' function automatically downloads and initializes the pretrained weights\n",
    "model = timm.create_model(f'{base_model}.{version}', pretrained=True, num_classes=len(class_names))\n",
    "\n",
    "# Set the device and data type for the model\n",
    "model = model.to(device=device, dtype=dtype)\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "model.device = device\n",
    "model.name = f'{base_model}.{version}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef872ed2",
   "metadata": {},
   "source": [
    "### Summarizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bf575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input to the model\n",
    "test_inp = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "# Get a summary of the model as a Pandas DataFrame\n",
    "summary_df = markdown_to_pandas(f\"{get_module_summary(model, [test_inp])}\")\n",
    "\n",
    "# Filter the summary to only contain Conv2d layers and the model\n",
    "summary_df = summary_df[(summary_df.index == 0) | (summary_df['Type'] == 'Conv2d')]\n",
    "\n",
    "# Remove the column \"Contains Uninitialized Parameters?\"\n",
    "summary_df.drop('Contains Uninitialized Parameters?', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde62928",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a728afe-81b1-4783-890c-20d6ae5e3037",
   "metadata": {},
   "source": [
    "### Training-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e0bd3-5fb5-4a4e-91e4-ac80fc858d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the image paths\n",
    "random.shuffle(img_paths)\n",
    "\n",
    "# Define the percentage of the images that should be used for training, validation, and testing\n",
    "train_pct = 0.8  # 80% for training\n",
    "val_pct = 0.1    # 10% for validation\n",
    "test_pct = 0.1   # 10% for testing\n",
    "\n",
    "# Calculate the index at which to split the subset of image paths\n",
    "train_split = int(len(img_paths) * train_pct)\n",
    "val_split = train_split + int(len(img_paths) * val_pct)\n",
    "\n",
    "# Split the subset of image paths into training, validation, and testing sets\n",
    "train_paths = img_paths[:train_split]\n",
    "val_paths = img_paths[train_split:val_split]\n",
    "test_paths = img_paths[val_split:]\n",
    "\n",
    "# Print the number of images in the training, validation, and testing sets\n",
    "summary_df = pd.Series({\n",
    "    \"Training Samples:\": len(train_paths),\n",
    "    \"Validation Samples:\": len(val_paths),\n",
    "    \"Testing Samples:\": len(test_paths)\n",
    "}).to_frame()\n",
    "\n",
    "# Display the summary DataFrame without showing the columns\n",
    "summary_df.style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5de79-f9fe-43b3-b3e2-86a6106fded7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778f9a5-ceb5-435d-b7ad-796e56fa5439",
   "metadata": {},
   "source": [
    "#### Set training image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "bb1c6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sz = 224 # image size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d5e5d-6e04-4f8c-afa1-2b1caa81c382",
   "metadata": {},
   "source": [
    "#### Initialize image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "a9abeb04-6c4c-47b3-aa1f-ad0704e27b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fill color for padding images\n",
    "fill = (0,0,0)\n",
    "\n",
    "# Create a `ResizeMax` object\n",
    "resize_max = ResizeMax(max_sz=train_sz)\n",
    "\n",
    "# Create a `PadSquare` object\n",
    "pad_square = PadSquare(shift=True, fill=fill)\n",
    "\n",
    "# # Create a TrivialAugmentWide object\n",
    "trivial_aug = transforms.TrivialAugmentWide(fill=fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c83589-cbe4-45b8-b901-cad7970ede12",
   "metadata": {},
   "source": [
    "#### Test the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69fd19-43d0-421c-bf2f-8e877ad67da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = Image.open(img_paths[11])\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac465c28-92b4-4ea8-be26-eb5171310e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the image\n",
    "augmented_img = trivial_aug(sample_img)\n",
    "\n",
    "# Resize the image\n",
    "resized_img = resize_max(augmented_img)\n",
    "\n",
    "# Pad the image\n",
    "padded_img = pad_square(resized_img)\n",
    "\n",
    "# Ensure the padded image is the target size\n",
    "resize = transforms.Resize([train_sz] * 2, antialias=True)\n",
    "resized_padded_img = resize(padded_img)\n",
    "\n",
    "# Display the annotated image\n",
    "display(resized_padded_img)\n",
    "\n",
    "pd.Series({\n",
    "    \"Source Image:\": sample_img.size,\n",
    "    \"Resized Image:\": resized_img.size,\n",
    "    \"Padded Image:\": padded_img.size,\n",
    "    \"Resized Padded Image:\": resized_padded_img.size,\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1118769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "74bbd946-c7a2-4bce-a35b-8d042aafbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for handling images.\n",
    "    \n",
    "    This class extends PyTorch's Dataset and is designed to work with image data. \n",
    "    It supports loading images, and applying transformations.\n",
    "\n",
    "    Attributes:\n",
    "        img_paths (list): List of image file paths.\n",
    "        class_to_idx (dict): Dictionary mapping class names to class indices.\n",
    "        transforms (callable, optional): Transformations to be applied to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_paths, class_to_idx, transforms=None):\n",
    "        \"\"\"\n",
    "        Initializes the ImageDataset with image keys and other relevant information.\n",
    "        \n",
    "        Args:\n",
    "            img_paths (list): List of image file paths.\n",
    "            class_to_idx (dict): Dictionary mapping class names to class indices.\n",
    "            transforms (callable, optional): Transformations to be applied to the images.\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self._img_paths = img_paths\n",
    "        self._class_to_idx = class_to_idx\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of items in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self._img_paths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset at the specified index.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its corresponding label.\n",
    "        \"\"\"\n",
    "        img_path = self._img_paths[index]\n",
    "        image, label = self._load_image(img_path)\n",
    "        \n",
    "        # Applying transformations if specified\n",
    "        if self._transforms:\n",
    "            image = self._transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _load_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Loads an image from the provided image path.\n",
    "        \n",
    "        Args:\n",
    "            img_path (string): Image path.\n",
    "            Returns:\n",
    "        tuple: A tuple containing the loaded image and its corresponding target data.\n",
    "        \"\"\"\n",
    "        # Load the image from the file path\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        return image, self._class_to_idx[img_path.parent.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4926ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "4a94b36c-e4d6-4218-9025-f6474647dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose transforms to resize and pad input images\n",
    "resize_pad_tfm = transforms.Compose([\n",
    "    resize_max, \n",
    "    # pad_square, # Padding image, use this if your model will learn representations in correct aspect ratio\n",
    "    transforms.Resize([train_sz] * 2, antialias=True)\n",
    "])\n",
    "\n",
    "# Compose transforms to sanitize bounding boxes and normalize input data\n",
    "final_tfms = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(*norm_stats),\n",
    "])\n",
    "\n",
    "# Define the transformations for training and validation datasets\n",
    "# Note: Data augmentation is performed only on the training dataset\n",
    "train_tfms = transforms.Compose([\n",
    "    # trivial_aug,\n",
    "    resize_pad_tfm, \n",
    "    # transforms.RandomRotation(90),             # Randomly rotate the image by 0, 90, 180, or 270 degrees\n",
    "    transforms.RandomHorizontalFlip(p=0.5),     # Randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.15),  # Random color adjustments\n",
    "    final_tfms\n",
    "])\n",
    "valid_tfms = transforms.Compose([\n",
    "    resize_pad_tfm, \n",
    "    final_tfms\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc82b0",
   "metadata": {},
   "source": [
    "### Initialize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a30a03-0ea8-4575-9237-9f444e364076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to class indices\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "# Instantiate the dataset using the defined transformations\n",
    "train_dataset = ImageDataset(train_paths, class_to_idx, train_tfms)\n",
    "valid_dataset = ImageDataset(val_paths, class_to_idx, valid_tfms)\n",
    "test_dataset = ImageDataset(test_paths, class_to_idx, valid_tfms)\n",
    "\n",
    "# Print the number of samples in the training and validation datasets\n",
    "pd.Series({\n",
    "    'Training dataset size:': len(train_dataset),\n",
    "    'Validation dataset size:': len(valid_dataset),\n",
    "    'Test dataset size:': len(test_dataset)}\n",
    ").to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43703140",
   "metadata": {},
   "source": [
    "### Inspect Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca4387",
   "metadata": {},
   "source": [
    "**Inspect training set sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for the first image in the training set\n",
    "print(f\"Label: {class_names[train_dataset[0][1]]}\")\n",
    "\n",
    "# Get the first image in the training set\n",
    "TF.to_pil_image(denorm_img_tensor(train_dataset[4][0], *norm_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55a9c2",
   "metadata": {},
   "source": [
    "**Inspect validation set sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9355df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for the first image in the validation set\n",
    "print(f\"Label: {class_names[valid_dataset[0][1]]}\")\n",
    "\n",
    "# Get the first image in the validation set\n",
    "TF.to_pil_image(denorm_img_tensor(valid_dataset[0][0], *norm_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eddfa",
   "metadata": {},
   "source": [
    "### Training Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "c2456a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496a3fd",
   "metadata": {},
   "source": [
    "### Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of worker processes for loading data. This should be the number of CPUs available.\n",
    "num_workers = multiprocessing.cpu_count()#//2\n",
    "\n",
    "# Define parameters for DataLoader\n",
    "data_loader_params = {\n",
    "    'batch_size': bs,  # Batch size for data loading\n",
    "    # 'num_workers': num_workers,  # Number of subprocesses to use for data loading\n",
    "    # 'persistent_workers': True,  # If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.\n",
    "    'pin_memory': 'cuda' in device,  # If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.\n",
    "    'pin_memory_device': device if 'cuda' in device else '',  # Specifies the device where the data should be loaded. Commonly set to use the GPU.\n",
    "}\n",
    "\n",
    "# Create DataLoader for training data. Data is shuffled for every epoch.\n",
    "train_dataloader = DataLoader(train_dataset, **data_loader_params, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation data. Shuffling is not necessary for validation data.\n",
    "valid_dataloader = DataLoader(valid_dataset, **data_loader_params)\n",
    "test_dataloader = DataLoader(test_dataset, **data_loader_params)\n",
    "\n",
    "# Print the number of batches in the training and validation DataLoaders\n",
    "print(f'Number of batches in train DataLoader: {len(train_dataloader)}')\n",
    "print(f'Number of batches in validation DataLoader: {len(valid_dataloader)}')\n",
    "print(f'Number of batches in test DataLoader: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957cdd",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513549f",
   "metadata": {},
   "source": [
    "### Define the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3aa9c-0eab-4d5b-b231-095f7ec52d73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "07240316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to run a single training/validation epoch\n",
    "# def run_epoch(model, dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch_id, is_training):\n",
    "#     # Set model to training mode if 'is_training' is True, else set to evaluation mode\n",
    "#     model.train() if is_training else model.eval()\n",
    "    \n",
    "#     # Reset the performance metric\n",
    "#     metric.reset()\n",
    "#     # Initialize the average loss for the current epoch \n",
    "#     epoch_loss = 0\n",
    "#     # Initialize progress bar with total number of batches in the dataloader\n",
    "#     progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")\n",
    "    \n",
    "#     # Iterate over data batches\n",
    "#     for batch_id, (inputs, targets) in enumerate(dataloader):\n",
    "#         # Move inputs and targets to the specified device (e.g., GPU)\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         # Enables gradient calculation if 'is_training' is True\n",
    "#         with torch.set_grad_enabled(is_training):\n",
    "#             # Automatic Mixed Precision (AMP) context manager for improved performance\n",
    "#             with autocast(torch.device(device).type):\n",
    "#                 outputs = model(inputs) # Forward pass\n",
    "#                 loss = torch.nn.functional.cross_entropy(outputs, targets) # Compute loss\n",
    "        \n",
    "#         # Update the performance metric\n",
    "#         metric.update(outputs.detach().cpu(), targets.detach().cpu())\n",
    "        \n",
    "#         # If in training mode\n",
    "#         if is_training:\n",
    "#             if scaler:\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(optimizer)\n",
    "#                 old_scaler = scaler.get_scale()\n",
    "#                 scaler.update()\n",
    "#                 new_scaler = scaler.get_scale()\n",
    "#                 if new_scaler >= old_scaler:\n",
    "#                     lr_scheduler.step()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 lr_scheduler.step()\n",
    "                \n",
    "#             optimizer.zero_grad()\n",
    "        \n",
    "#         loss_item = loss.item()\n",
    "#         epoch_loss += loss_item\n",
    "#         # Update progress bar\n",
    "#         progress_bar.set_postfix(accuracy=metric.compute().item(), \n",
    "#                                  loss=loss_item, \n",
    "#                                  avg_loss=epoch_loss/(batch_id+1), \n",
    "#                                  lr=lr_scheduler.get_last_lr()[0] if is_training else \"\")\n",
    "#         progress_bar.update()\n",
    "        \n",
    "#         # If loss is NaN or infinity, stop training\n",
    "#         if is_training:\n",
    "#             stop_training_message = f\"Loss is NaN or infinite at epoch {epoch_id}, batch {batch_id}. Stopping training.\"\n",
    "#             assert not math.isnan(loss_item) and math.isfinite(loss_item), stop_training_message\n",
    "        \n",
    "#     progress_bar.close()\n",
    "#     return epoch_loss / (batch_id + 1)\n",
    "\n",
    "# # Main training loop\n",
    "# def train_loop(model, train_dataloader, valid_dataloader, optimizer, metric, lr_scheduler, device, epochs, checkpoint_path, use_scaler=False):\n",
    "#     # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
    "#     scaler = GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "#     best_loss = float('inf')\n",
    "\n",
    "#     # Iterate over each epoch\n",
    "#     for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "#         # Run training epoch and compute training loss\n",
    "#         train_loss = run_epoch(model, train_dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch, is_training=True)\n",
    "#         # Run validation epoch and compute validation loss\n",
    "#         with torch.no_grad():\n",
    "#             valid_loss = run_epoch(model, valid_dataloader, None, metric, None, device, scaler, epoch, is_training=False)\n",
    "        \n",
    "#         # If current validation loss is lower than the best one so far, save model and update best loss\n",
    "#         if valid_loss < best_loss:\n",
    "#             best_loss = valid_loss\n",
    "#             metric_value = metric.compute().item()\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "#             training_metadata = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'valid_loss': valid_loss, \n",
    "#                 'metric_value': metric_value,\n",
    "#                 'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "#                 'model_architecture': model.name\n",
    "#             }\n",
    "            \n",
    "#             # Save best_loss and metric_value in a JSON file\n",
    "#             with open(Path(checkpoint_path.parent/'training_metadata.json'), 'w') as f:\n",
    "#                 json.dump(training_metadata, f)\n",
    "\n",
    "#     # If the device is a GPU, empty the cache\n",
    "#     if device.type != 'cpu':\n",
    "#         getattr(torch, device.type).empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c00cf-ae7a-48b6-a44d-1ca10e832ce1",
   "metadata": {},
   "source": [
    "### Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "774020bf-2b6c-41df-96c3-e7804db2f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies, checkpoint_path):\n",
    "    # Generate an array of epoch numbers\n",
    "    epoch_range = np.arange(1, epochs + 1)\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Subplot for Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_range, train_losses, label='Train Loss', color='blue', marker='o')\n",
    "    plt.plot(epoch_range, valid_losses, label='Valid Loss', color='orange', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_range, train_accuracies, label='Train Accuracy', color='blue', marker='o')\n",
    "    plt.plot(epoch_range, valid_accuracies, label='Valid Accuracy', color='orange', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plots_dir = os.path.join(checkpoint_path.parent, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    plot_filename = os.path.join(plots_dir, 'training_results.png')\n",
    "    plt.savefig(plot_filename, dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"Plots saved at: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "eef5709a-31d3-4b10-a4f5-0c87679c5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch_id, is_training, loss_fn):\n",
    "    # Set model to training mode if 'is_training' is True, else set to evaluation mode\n",
    "    model.train() if is_training else model.eval()\n",
    "    \n",
    "    # Reset the performance metric\n",
    "    metric.reset()\n",
    "    # Initialize the average loss for the current epoch \n",
    "    epoch_loss = 0\n",
    "    # Initialize progress bar with total number of batches in the dataloader\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")\n",
    "    \n",
    "    for batch_id, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move inputs and targets to the specified device (e.g., GPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Enables gradient calculation if 'is_training' is True\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            # Automatic Mixed Precision (AMP) context manager for improved performance\n",
    "            with autocast(torch.device(device).type):\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = loss_fn(outputs, targets)  # Compute loss\n",
    "    \n",
    "        # Update the performance metric\n",
    "        metric.update(outputs.detach().cpu(), targets.detach().cpu())\n",
    "        \n",
    "        # If in training mode\n",
    "        if is_training:\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                old_scaler = scaler.get_scale()\n",
    "                scaler.update()\n",
    "                new_scaler = scaler.get_scale()\n",
    "                if new_scaler >= old_scaler:\n",
    "                    lr_scheduler.step()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        epoch_loss += loss_item\n",
    "        accuracy = metric.compute().item()  # Calculate current accuracy\n",
    "        progress_bar.set_postfix(accuracy=accuracy, \n",
    "                                 loss=loss_item, \n",
    "                                 avg_loss=epoch_loss/(batch_id+1), \n",
    "                                 lr=lr_scheduler.get_last_lr()[0] if is_training else \"\")\n",
    "        progress_bar.update()\n",
    "        \n",
    "        # If loss is NaN or infinity, stop training\n",
    "        assert not math.isnan(loss_item) and math.isfinite(loss_item), f\"Loss is NaN or infinite at epoch {epoch_id}, batch {batch_id}. Stopping training.\"\n",
    "\n",
    "    progress_bar.close()\n",
    "    return epoch_loss / (batch_id + 1), accuracy  # Return loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "0750c4f3-7af8-461b-bcde-b03a8b23bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main training loop\n",
    "# def train_loop(model, train_dataloader, valid_dataloader, optimizer, metric, lr_scheduler, device, epochs, checkpoint_path, use_scaler=False):\n",
    "#     # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
    "#     scaler = GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "#     best_loss = float('inf')\n",
    "    \n",
    "#     # Iterate over each epoch\n",
    "#     for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "#         # Run training epoch and compute training loss\n",
    "#         train_loss, train_accuracy = run_epoch(model, train_dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch, is_training=True)\n",
    "#         # Run validation epoch and compute validation loss\n",
    "#         with torch.no_grad():\n",
    "#             valid_loss, valid_accuracy = run_epoch(model, valid_dataloader, None, metric, None, device, scaler, epoch, is_training=False)\n",
    "\n",
    "#         # If current validation loss is lower than the best one so far, save model and update best loss\n",
    "#         if valid_loss < best_loss:\n",
    "#             best_loss = valid_loss\n",
    "            \n",
    "#             # Save model state and metrics to JSON file\n",
    "#             training_metadata = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'valid_loss': valid_loss,\n",
    "#                 'train_accuracy': train_accuracy,\n",
    "#                 'valid_accuracy': valid_accuracy,\n",
    "#                 'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "#                 'model_architecture': model.name,\n",
    "#             }\n",
    "            \n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "#             # Save best_loss and metric_value in a JSON file\n",
    "#             with open(Path(checkpoint_path.parent / 'training_metadata.json'), 'w') as f:\n",
    "#                 json.dump(training_metadata, f)\n",
    "    \n",
    "#     # If the device is a GPU, empty the cache\n",
    "#     if device.type != 'cpu':\n",
    "#         getattr(torch, device.type).empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f62bd234-73ea-49aa-8645-cb6aa4f4932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataloader, valid_dataloader, optimizer, metric, lr_scheduler, device, epochs, checkpoint_path, loss_fn, use_scaler=False):\n",
    "    # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
    "    scaler = GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Initialize Weights and Biases (W&B)\n",
    "    wandb.init(project='timm-models', anonymous=\"allow\", name=model.__class__.__name__, config={\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "        'architecture': model.__class__.__name__,\n",
    "    })\n",
    "\n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    # Iterate over each epoch\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # Run training epoch and compute training loss\n",
    "        train_loss, train_accuracy = run_epoch(model, train_dataloader, optimizer, metric, lr_scheduler, device, scaler, epoch, loss_fn=loss_fn, is_training=True)\n",
    "        # Run validation epoch and compute validation loss\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_accuracy = run_epoch(model, valid_dataloader, None, metric, None, device, scaler, epoch, loss_fn=loss_fn, is_training=False)\n",
    "        \n",
    "        # Append metrics for plotting later\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'valid_accuracy': valid_accuracy,\n",
    "            'learning_rate': lr_scheduler.get_last_lr()[0]\n",
    "        }, steps=epoch)\n",
    "        \n",
    "        # If current validation loss is lower than the best one so far, save model and update best loss\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            \n",
    "            # Save model state and metrics to JSON file\n",
    "            training_metadata = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'valid_accuracy': valid_accuracy,\n",
    "                'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "                'model_architecture': model.__class__.__name__,\n",
    "            }\n",
    "            \n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "            # Save best_loss and metric_value in a JSON file\n",
    "            with open(Path(checkpoint_path).parent / 'training_metadata.json', 'w') as f:\n",
    "                json.dump(training_metadata, f, indent=4)\n",
    "    \n",
    "    # If the device is a GPU, empty the cache\n",
    "    if device.type != 'cpu':\n",
    "        getattr(torch, device.type).empty_cache()\n",
    "    \n",
    "    # Plot the training and validation metrics\n",
    "    plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies, checkpoint_path)\n",
    "    \n",
    "    # Finish W&B logging session\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c4f56-3788-44e0-9b0a-5b3896f60846",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "9d40d923-4ead-4a7c-b1a6-cc094515cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Trainer:\n",
    "#     def __init__(self, model, train_dataloader, valid_dataloader, optimizer, metric, lr_scheduler, device, epochs, checkpoint_path, use_scaler=False, class_weights=None, clear_output=True, display=True, patience_counter=3):\n",
    "#         self.model = model\n",
    "#         self.train_dataloader = train_dataloader\n",
    "#         self.valid_dataloader = valid_dataloader\n",
    "#         self.optimizer = optimizer\n",
    "#         self.metric = metric\n",
    "#         self.lr_scheduler = lr_scheduler\n",
    "#         self.device = device\n",
    "#         self.epochs = epochs\n",
    "#         self.checkpoint_path = checkpoint_path\n",
    "#         self.use_scaler = use_scaler\n",
    "        \n",
    "#         # Initialize a gradient scaler for mixed-precision training if applicable\n",
    "#         self.scaler = GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "        \n",
    "#         # Store class weights if provided\n",
    "#         self.class_weights = class_weights\n",
    "\n",
    "#     def run_epoch(self, dataloader, is_training):\n",
    "#         self.model.train() if is_training else self.model.eval()\n",
    "#         self.metric.reset()\n",
    "#         epoch_loss = 0\n",
    "        \n",
    "#         with tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\") as progress_bar:\n",
    "#             for batch_id, (inputs, targets) in enumerate(dataloader):\n",
    "#                 inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "#                 with torch.set_grad_enabled(is_training):\n",
    "#                     with autocast(torch.device(self.device).type):\n",
    "#                         outputs = self.model(inputs)  # Forward pass\n",
    "\n",
    "#                         # Compute loss with class weights if provided\n",
    "#                         if self.class_weights is not None:\n",
    "#                             loss = torch.nn.functional.cross_entropy(outputs, targets, weight=self.class_weights)\n",
    "#                         else:\n",
    "#                             loss = torch.nn.functional.cross_entropy(outputs, targets)  # Compute loss without weights\n",
    "\n",
    "#                 # Update the performance metric\n",
    "#                 self.metric.update(outputs.detach().cpu(), targets.detach().cpu())\n",
    "\n",
    "#                 # If in training mode\n",
    "#                 if is_training:\n",
    "#                     if self.scaler:\n",
    "#                         self.scaler.scale(loss).backward()\n",
    "#                         self.scaler.step(self.optimizer)\n",
    "#                         old_scaler = self.scaler.get_scale()\n",
    "#                         self.scaler.update()\n",
    "#                         new_scaler = self.scaler.get_scale()\n",
    "#                         if new_scaler >= old_scaler:\n",
    "#                             self.lr_scheduler.step()\n",
    "#                     else:\n",
    "#                         loss.backward()\n",
    "#                         self.optimizer.step()\n",
    "#                         self.lr_scheduler.step()\n",
    "\n",
    "#                     self.optimizer.zero_grad()\n",
    "\n",
    "#                 loss_item = loss.item()\n",
    "#                 epoch_loss += loss_item\n",
    "#                 accuracy = self.metric.compute().item()  # Calculate current accuracy\n",
    "                \n",
    "#                 # Update progress bar with metrics\n",
    "#                 progress_bar.set_postfix(accuracy=accuracy,\n",
    "#                                           loss=loss_item,\n",
    "#                                           avg_loss=epoch_loss / (batch_id + 1),\n",
    "#                                           lr=self.lr_scheduler.get_last_lr()[0] if is_training else \"\")\n",
    "#                 progress_bar.update()\n",
    "\n",
    "#                 assert not math.isnan(loss_item) and math.isfinite(loss_item), f\"Loss is NaN or infinite at epoch {epoch_id}, batch {batch_id}. Stopping training.\"\n",
    "\n",
    "#         return epoch_loss / (batch_id + 1), accuracy  # Return loss and accuracy\n",
    "\n",
    "#     def train(self):\n",
    "#         best_loss = float('inf')\n",
    "        \n",
    "#         training_metrics = {}\n",
    "#         df_metrics = pd.DataFrame()\n",
    "#         training_state = []\n",
    "        \n",
    "#         # Wandb Visualization\n",
    "#         wandb.init(project=\"timm-models\", anonymous=\"allow\") \n",
    "\n",
    "#         for epoch in tqdm(range(self.epochs), desc=\"Epochs\"):\n",
    "#             train_loss, train_accuracy = self.run_epoch(self.train_dataloader, is_training=True)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 valid_loss, valid_accuracy = self.run_epoch(self.valid_dataloader, is_training=False)\n",
    "\n",
    "#             # Save metrics for plotting later\n",
    "#             training_metrics[epoch] = {\n",
    "#                 'train': {'loss': train_loss, 'accuracy': train_accuracy},\n",
    "#                 'validation': {'loss': valid_loss, 'accuracy': valid_accuracy}\n",
    "#             }\n",
    "#             training_state.append({\n",
    "#                     'epoch': epoch,\n",
    "#                     'train_loss': train_loss,\n",
    "#                     'valid_loss': valid_loss,\n",
    "#                     'train_accuracy': train_accuracy,\n",
    "#                     'valid_accuracy': valid_accuracy\n",
    "#             })\n",
    "            \n",
    "\n",
    "#             # If current validation loss is lower than the best one so far\n",
    "#             if valid_loss < best_loss:\n",
    "#                 best_loss = valid_loss\n",
    "#                 patience_counter = 0\n",
    "#                 # Save model state and metrics to JSON file\n",
    "#                 training_metadata = {\n",
    "#                     'epoch': epoch,\n",
    "#                     'train_loss': train_loss,\n",
    "#                     'valid_loss': valid_loss,\n",
    "#                     'train_accuracy': train_accuracy,\n",
    "#                     'valid_accuracy': valid_accuracy,\n",
    "#                     'learning_rate': self.lr_scheduler.get_last_lr()[0],\n",
    "#                     'model_architecture': self.model.name,\n",
    "#                 }\n",
    "                \n",
    "#                 torch.save(self.model.state_dict(), self.checkpoint_path)\n",
    "                \n",
    "#                  # Save metadata to JSON\n",
    "#                 with open(Path(self.checkpoint_path).parent / 'training_metadata.json', 'w') as f:\n",
    "#                     json.dump(training_metrics, f, indent=4)  # Save the list, not individual records\n",
    "                    \n",
    "#             elif valid_loss == best_loss:\n",
    "#                 patience_counter += 1  # Increment counter if validation loss is the same\n",
    "                \n",
    "#                 # Check if patience limit has been reached\n",
    "#                 if patience_counter >= 3:  # You can adjust this value as needed\n",
    "#                     print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "#                     break\n",
    "\n",
    "#             df_row = pd.DataFrame([training_metadata])\n",
    "#             df_metrics = pd.concat([df_metrics, df_row], ignore_index=True)\n",
    "            \n",
    "#             if hasattr(self, 'clear_output') and hasattr(self, 'display'):  # Only if running in a Jupyter environment\n",
    "#                 if self.clear_output and self.display:\n",
    "#                     self.clear_output(wait=True)\n",
    "#                     self.display(pd.DataFrame(training_state))\n",
    "\n",
    "#             # Log metrics to wandb\n",
    "#             wandb.log({\n",
    "#                 \"train_loss\": train_loss,\n",
    "#                 \"valid_loss\": valid_loss,\n",
    "#                 \"train_accuracy\": train_accuracy,\n",
    "#                 \"valid_accuracy\": valid_accuracy,\n",
    "#                 \"learning_rate\": self.lr_scheduler.get_last_lr()[0]\n",
    "#             }, step=epoch)  # Log with epoch as step\n",
    "        \n",
    "#         wandb.finish()\n",
    "        \n",
    "#         # Plotting results after training completes\n",
    "#         self.plot_results(training_metrics)\n",
    "\n",
    "#     def plot_results(self, training_metrics):\n",
    "#         epochs = list(training_metrics.keys())\n",
    "#         train_losses = [metrics['train']['loss'] for metrics in training_metrics.values()]\n",
    "#         val_losses = [metrics['validation']['loss'] for metrics in training_metrics.values()]\n",
    "#         train_accuracies = [metrics['train']['accuracy'] for metrics in training_metrics.values()]\n",
    "#         val_accuracies = [metrics['validation']['accuracy'] for metrics in training_metrics.values()]\n",
    "\n",
    "#         plt.figure(figsize=(14, 6))\n",
    "\n",
    "#         # Plot training and evaluation losses\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.plot(epochs, train_losses, label='Train Loss', color='red', marker='o')\n",
    "#         plt.plot(epochs, val_losses, label='Eval Loss', color='orange', marker='o')\n",
    "#         plt.xlabel('Epoch', fontsize=12)\n",
    "#         plt.ylabel('Loss', fontsize=12)\n",
    "#         plt.title('Training and Evaluation Loss', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "#         # Plot training and evaluation accuracies\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue', marker='o')\n",
    "#         plt.plot(epochs, val_accuracies, label='Eval Accuracy', color='green', marker='o')\n",
    "#         plt.xlabel('Epoch', fontsize=12)\n",
    "#         plt.ylabel('Accuracy', fontsize=12)\n",
    "#         plt.title('Training and Evaluation Accuracies', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "#         # Adjust layout and spacing\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         # Save the figure with high resolution\n",
    "#         plt.savefig(f\"{self.checkpoint_path.parent}/train_and_eval.jpg\", dpi=300)\n",
    "\n",
    "#         # Show the plot\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8eeeae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set the Model Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create a directory to store the checkpoints if it does not already exist\n",
    "checkpoint_dir = Path(project_dir/f\"{timestamp}\")\n",
    "\n",
    "# Create the checkpoint directory if it does not already exist\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The model checkpoint path\n",
    "checkpoint_path = checkpoint_dir/f\"{model.name}.pth\"\n",
    "\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358ec8c-6df2-4309-a92a-22b74db1e865",
   "metadata": {},
   "source": [
    "### Saving the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3539c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class labels\n",
    "class_labels = {\"classes\": list(class_names)}\n",
    "\n",
    "# Set file path\n",
    "class_labels_path = checkpoint_dir/f\"{dataset_name}-classes.json\"\n",
    "\n",
    "# Save class labels in JSON format\n",
    "with open(class_labels_path, \"w\") as write_file:\n",
    "    json.dump(class_labels, write_file)\n",
    "    \n",
    "print(class_labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9982e4-9936-45bd-86d5-616322091775",
   "metadata": {},
   "source": [
    "### Calculating Class Weights for Imbalanced Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5b5b7-be64-472f-91e1-018cf98833f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(train_dataset, class_to_idx):\n",
    "    \"\"\"\n",
    "    Calculate class weights for imbalanced datasets based on the training data.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (Dataset): The training dataset containing images and labels.\n",
    "        class_to_idx (dict): Dictionary mapping class names to class indices.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: An array containing the class weights.\n",
    "    \"\"\"\n",
    "    # Create a list of all class indices for images in the training dataset\n",
    "    class_counts = Counter([class_to_idx[Path(img_path).parent.name] for img_path in train_dataset._img_paths])\n",
    "    \n",
    "    # Get total number of samples\n",
    "    total_samples = len(train_dataset)\n",
    "\n",
    "    # Compute the class weights: inverse frequency\n",
    "    class_weights = {class_idx: total_samples / count for class_idx, count in class_counts.items()}\n",
    "    \n",
    "    # Normalize the weights to sum up to 1 (optional)\n",
    "    total_weight_sum = sum(class_weights.values())\n",
    "    class_weights = {class_idx: weight / total_weight_sum for class_idx, weight in class_weights.items()}\n",
    "\n",
    "    # Convert the dictionary to a list of weights where index represents the class\n",
    "    weights_list = np.zeros(len(class_to_idx))\n",
    "    for class_idx, weight in class_weights.items():\n",
    "        weights_list[class_idx] = weight\n",
    "        \n",
    "    return weights_list, class_counts  # Return both weights and counts\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights, class_counts = calculate_class_weights(train_dataset, class_to_idx)\n",
    "\n",
    "# Create a mapping of class indices to weights for better visualization\n",
    "class_index_to_weight = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=dtype).to(device)\n",
    "\n",
    "# Print class counts and corresponding weights\n",
    "print(\"Class Counts:\", class_counts)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "print(\"Class Weights Tensor:\", class_weights_tensor)\n",
    "\n",
    "# Optionally, print in a more structured way\n",
    "print(\"\\nClass Index  | Class Count  | Class Weight\")\n",
    "for idx in range(len(class_weights)):\n",
    "    count = class_counts[idx] if idx in class_counts else 0\n",
    "    weight = class_weights[idx]\n",
    "    print(f\"{idx:<12} | {count:<12} | {weight:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05437529",
   "metadata": {},
   "source": [
    "### Configure the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "5a61492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for the model\n",
    "lr = 2e-05\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# AdamW optimizer; includes weight decay for regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "# Learning rate scheduler; adjusts the learning rate during training\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                   max_lr=lr, \n",
    "                                                   total_steps=epochs*len(train_dataloader))\n",
    "# Define the loss function with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Performance metric: Multiclass Accuracy\n",
    "metric = MulticlassAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7973515",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035388be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model=model, \n",
    "           train_dataloader=train_dataloader, \n",
    "           valid_dataloader=valid_dataloader, \n",
    "           optimizer=optimizer, \n",
    "           metric=metric, \n",
    "           lr_scheduler=lr_scheduler, \n",
    "           device=torch.device(device), \n",
    "           epochs=epochs, \n",
    "           checkpoint_path=checkpoint_path, \n",
    "           loss_fn=loss_fn,\n",
    "           use_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "43be6e74-5bb4-4087-91e8-83849fc5fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Usage example (assuming other variables are initialized)\n",
    "# trainer = Trainer(model=model,\n",
    "#                   train_dataloader=train_dataloader,\n",
    "#                   valid_dataloader=valid_dataloader,\n",
    "#                   optimizer=optimizer,\n",
    "#                   metric=metric,\n",
    "#                   lr_scheduler=lr_scheduler,\n",
    "#                   device=torch.device(device),\n",
    "#                   epochs=epochs,\n",
    "#                   checkpoint_path=checkpoint_path,\n",
    "#                   use_scaler=True,\n",
    "#                  )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "4b546079-5ab6-4bed-9d94-9720a4cd9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have already defined your model, dataloaders, optimizer, etc.\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     valid_dataloader=valid_dataloader,\n",
    "#     optimizer=optimizer,\n",
    "#     metric=metric,  # Replace with your desired metric\n",
    "#     lr_scheduler=lr_scheduler,\n",
    "#     device=torch.device(device), \n",
    "#     epochs=10,\n",
    "#     checkpoint_path=checkpoint_path,\n",
    "#     use_scaler=True,  # Enable mixed-precision training if needed\n",
    "#     clear_output=True,  # Clear output before printing new epoch stats\n",
    "#     display=True,  # Display training stats at the end of each epoch\n",
    "#     patience_counter=3,\n",
    "#     class_weights=class_weights_tensor  # Use early stopping\n",
    "# )\n",
    "\n",
    "# # Start the training process\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478366d-e213-4b89-bdb0-dcf4f1fed885",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff88d8f-fcbc-4ee6-b102-f40a8c27f044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5296a22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Making Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an item from the validation set\n",
    "test_file = val_paths[0]\n",
    "\n",
    "# Open the test file\n",
    "test_img = Image.open(test_file).convert('RGB')\n",
    "\n",
    "# Set the minimum input dimension for inference \n",
    "input_img = resize_img(test_img, target_sz=train_sz, divisor=1)\n",
    "\n",
    "# Convert the image to a normalized tensor and move it to the device\n",
    "img_tensor = pil_to_tensor(input_img, *norm_stats).to(device=device)\n",
    "\n",
    "# Make a prediction with the model\n",
    "with torch.no_grad():\n",
    "    pred = model(img_tensor)\n",
    "        \n",
    "# Scale the model predictions to add up to 1\n",
    "pred_scores = torch.softmax(pred, dim=1)\n",
    "\n",
    "# Get the highest confidence score\n",
    "confidence_score = pred_scores.max()\n",
    "\n",
    "# Get the class index with the highest confidence score and convert it to the class name\n",
    "pred_class = class_names[torch.argmax(pred_scores)]\n",
    "\n",
    "# Display the image\n",
    "display(test_img)\n",
    "\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "\n",
    "# Print the prediction data as a Pandas DataFrame for easy formatting\n",
    "confidence_score_df = pd.DataFrame({\n",
    "    'Confidence Score':{\n",
    "        name:f'{score*100:.2f}%' for name, score in zip(class_names, pred_scores.cpu().numpy()[0])\n",
    "    }\n",
    "})\n",
    "confidence_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc6fd0",
   "metadata": {},
   "source": [
    "### Testing the Model on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1262c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'pexels-elina-volkova-16191659.jpg'\n",
    "# file_name = 'pexels-joshua-roberts-12922530.jpg'\n",
    "# file_name = 'pexels-luke-barky-2899727.jpg'\n",
    "# file_name = 'pexels-ketut-subiyanto-4584599.jpg'\n",
    "# file_name = 'pexels-nataliya-vaitkevich-5411990.jpg'\n",
    "# file_name = 'pexels-darina-belonogova-7886753.jpg'\n",
    "# file_name = 'pexels-katrin-bolovtsova-6706013.jpg'\n",
    "# file_name = 'pexels-leo-vinicius-3714450.jpg'\n",
    "# file_name = 'pexels-diva-plavalaguna-6937816.jpg'\n",
    "\n",
    "test_img_url = f\"https://huggingface.co/datasets/cj-mills/pexel-hand-gesture-test-images/resolve/main/{file_name}\"\n",
    "test_img_path = Path(file_name)\n",
    "\n",
    "if test_img_path.is_file():\n",
    "    print(\"Image already exists.\")\n",
    "else:\n",
    "    urllib.request.urlretrieve(test_img_url, test_img_path)\n",
    "    print(\"Image downloaded.\")\n",
    "\n",
    "test_img = Image.open(test_img_path)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum input dimension for inference \n",
    "infer_sz = train_sz\n",
    "\n",
    "inp_img = resize_img(test_img.copy(), infer_sz)\n",
    "\n",
    "# Convert the image to a normalized tensor and move it to the device\n",
    "img_tensor = pil_to_tensor(inp_img, *norm_stats).to(device=device)\n",
    "\n",
    "# Make a prediction with the model\n",
    "with torch.no_grad():\n",
    "    pred = model(img_tensor)\n",
    "        \n",
    "# Scale the model predictions to add up to 1\n",
    "pred_scores = torch.softmax(pred, dim=1)\n",
    "\n",
    "# Get the highest confidence score\n",
    "confidence_score = pred_scores.max()\n",
    "\n",
    "# Get the class index with the highest confidence score and convert it to the class name\n",
    "pred_class = class_names[torch.argmax(pred_scores)]\n",
    "\n",
    "# Display the image\n",
    "display(test_img)\n",
    "\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "\n",
    "# Print the prediction data as a Pandas DataFrame for easy formatting\n",
    "confidence_score_df = pd.DataFrame({\n",
    "    'Confidence Score':{\n",
    "        name:f'{score*100:.2f}%' for name, score in zip(class_names, pred_scores.cpu().numpy()[0])\n",
    "    }\n",
    "})\n",
    "confidence_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9a492-c954-47ce-b1b5-32e6243d64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum input dimension for inference \n",
    "infer_sz = train_sz\n",
    "\n",
    "inp_img = resize_img(test_img.copy(), infer_sz)\n",
    "\n",
    "# Convert the image to a normalized tensor and move it to the device\n",
    "img_tensor = pil_to_tensor(inp_img, *norm_stats).to(device=device)\n",
    "\n",
    "# Make a prediction with the model\n",
    "with torch.no_grad():\n",
    "    pred = model(img_tensor)\n",
    "        \n",
    "# Scale the model predictions to add up to 1\n",
    "pred_scores = torch.softmax(pred, dim=1)\n",
    "\n",
    "# Get the highest confidence score\n",
    "confidence_score = pred_scores.max()\n",
    "\n",
    "# Get the class index with the highest confidence score and convert it to the class name\n",
    "pred_class = class_names[torch.argmax(pred_scores)]\n",
    "\n",
    "# Display the image\n",
    "display(test_img)\n",
    "\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "\n",
    "# Print the prediction data as a Pandas DataFrame for easy formatting\n",
    "confidence_score_df = pd.DataFrame({\n",
    "    'Confidence Score':{\n",
    "        name:f'{score*100:.2f}%' for name, score in zip(class_names, pred_scores.cpu().numpy()[0])\n",
    "    }\n",
    "})\n",
    "confidence_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5afbe4-8210-4bc4-8476-04c242b38531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
